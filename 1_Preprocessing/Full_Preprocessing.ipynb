{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf49f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset downloaded from kaggle: https://www.kaggle.com/datasets/saurabhshahane/cyberbullying-dataset\n",
    "df = pd.read_csv(\"path/Dataset/twitter_parsed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74ac58",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7efa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat words List From https://www.kaggle.com/code/sndpkirwai/nlp-basic-text-preprocessing-steps/notebook + I added some more\n",
    "chat_words_str = \"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My ASS Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The ASS\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My Ass Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The Fuck\n",
    "WTG=Way To Go\n",
    "WUF=Where Are You From\n",
    "W8=Wait\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\"\n",
    "\n",
    "chat_words_str += \"\"\"\n",
    "AIGHT=Alright\n",
    "AYT=Alright\n",
    "BFF=Best Friends Forever\n",
    "BRO=Brother\n",
    "COZ=Because\n",
    "CUZ=Because\n",
    "DM=Direct Message\n",
    "DEETS=Details\n",
    "EM=Them\n",
    "EMO=Emotional\n",
    "FML=Fuck My Life\n",
    "FTW=For The Win\n",
    "FTL=For The Loss\n",
    "GRATS=Congratulations\n",
    "GR8=Great\n",
    "GTG=Got To Go\n",
    "G2G=Got To Go\n",
    "HBU=How About You\n",
    "HMU=Hit Me Up\n",
    "IDC=I Do Not Care\n",
    "IDK=I Do Not Know\n",
    "ILY=I Love You\n",
    "IMU=I Miss You\n",
    "JS=Just Saying\n",
    "JK=Just Kidding\n",
    "K=Okay\n",
    "KK=Okay\n",
    "L8=Late\n",
    "LUV=Love\n",
    "MSG=Message\n",
    "MYOB=Mind Your Own Business\n",
    "NBD=No Big Deal\n",
    "NGL=Not Gonna Lie\n",
    "NP=No Problem\n",
    "NSFW=Not Safe For Work\n",
    "NVM=Never Mind\n",
    "OBV=Obviously\n",
    "OMG=Oh My God\n",
    "OMFG=Oh My Fucking God\n",
    "PLS=Please\n",
    "PLZ=Please\n",
    "PPL=People\n",
    "RLY=Really\n",
    "RN=Right Now\n",
    "RU=Are You\n",
    "SMH=Shaking My Head\n",
    "SRSLY=Seriously\n",
    "SUP=What's up\n",
    "TBH=To Be Honest\n",
    "TGIF=Thank God It's Friday\n",
    "TMI=Too Much Information\n",
    "TTYS=Talk To You Soon\n",
    "TY=Thank You\n",
    "TYT=Take Your Time\n",
    "VC=Voice Chat\n",
    "WBU=What About You\n",
    "WDYM=What Do You Mean\n",
    "WTH=What The Hell\n",
    "XOXO=Hugs and Kisses\n",
    "YA=You\n",
    "YO=Hey\n",
    "YOLO=You Only Live Once\n",
    "YUP=Yes\n",
    "ZOMG=Oh My God\n",
    "IDGAF=I Do Not Give A Fuck\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca211f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one minute be right back,'"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to convert chat words from the same notebook\n",
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    if line.strip() != \"\":\n",
    "        cw = line.split(\"=\")[0].lower()\n",
    "        cw_expanded = line.split(\"=\")[1].lower()\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "\n",
    "chat_words_list = set(chat_words_list)\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        match = re.match(r\"([a-zA-Z]+)(\\W*)\", w)  # ex: \"brb,\" -> (\"brb\", \",\")\n",
    "        if match:\n",
    "            word, punctuation = match.groups()\n",
    "            lw = word.lower()\n",
    "            if lw in chat_words_list:\n",
    "                new_text.append(chat_words_map_dict[lw] + punctuation)\n",
    "            else:\n",
    "                new_text.append(w)\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "chat_words_conversion(\"one minute brb,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove mentions at the beginning and end\n",
    "def remove_boundary_users(text):\n",
    "    words = text.split()\n",
    "    # Remove <user> at the beginning\n",
    "    if words and words[0] == \"<user>\":\n",
    "        words = words[1:]\n",
    "    # Remove <user> at the end\n",
    "    if words and words[-1] == \"<user>\":\n",
    "        words = words[:-1]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131da59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are a loser <user> ! see you tomorrow with <num> euros ! and . jean you owe me <num>\n"
     ]
    }
   ],
   "source": [
    "def prepro(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize raw social media text.\n",
    "\n",
    "    Steps:\n",
    "     1. HTML artifacts replacement\n",
    "     2. Remove URLs, emojis, trailing hashtags\n",
    "     3. Mask mentions and inline hashtags\n",
    "     4. Handle punctuation and RT tokens\n",
    "     5. Normalize user tokens and repeated words\n",
    "     6. Anonymize numbers and specific names\n",
    "     7. Remove unwanted tokens and punctuation repeats\n",
    "     8. Linguistic corrections: expand contractions, convert chat words, fix spelling\n",
    "     9. Final cleanup: lowercase, boundary users, whitespace normalization\n",
    "    \"\"\"\n",
    "    # 1. HTML artifacts replacement\n",
    "    text = text.replace(\"&amp\", \"and\")\n",
    "\n",
    "    # 2. Fundamental removals: URLs, emojis, trailing hashtags\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = emoji.replace_emoji(text, \"\")\n",
    "    text = re.sub(r\"(\\s+#\\S+)+\\s*$\", \"\", text)\n",
    "\n",
    "    # 3. Mask mentions and remove '#' from inline hashtags\n",
    "    text = re.sub(r\"@\\w+\", \"<user>\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "\n",
    "    # 4. Keep strong punctuation only and remove RT tokens\n",
    "    text = re.sub(r\"[^\\w\\s<>.!?]\", \"\", text)\n",
    "    text = re.sub(r\"^RT\\s+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+(RT|RTvid)\\s*$\", \"\", text)\n",
    "    text = re.sub(r\"\\bRT\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 5. Normalize repeated <user> tokens and repeated words\n",
    "    text = re.sub(r'(<user>\\s*)+', '<user> ', text)\n",
    "    text = re.sub(r\"\\b(\\S+)( \\1\\b)+\", r\"\\1 \", text)\n",
    "\n",
    "    # 6. Anonymize numbers and specific names\n",
    "    text = re.sub(r'(?<=[a-zA-Z])\\d+(?=\\b)', '', text)\n",
    "    text = re.sub(r'\\d+(?:\\.\\d+)?(?:st|nd|rd|th|s|k|m|to|x|xs|cm|in|id)?', '<num>', text)\n",
    "    text = re.sub(r\"\\b(max|maxs|valentis|drasko|drasco|druitts|katandandre|katjia|khybar|kobane|oktars|tomz|nickis|nikkis|kat|katie|nikki|colin|emma|lyn|lynn|andre)\\b\", '<name>', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 7. Remove unnecessary tokens and punctuation repetitions\n",
    "    text = re.sub(r\"\\bmkr\\b\", \"\", text)\n",
    "    text = re.sub(r'([!?.])(?:\\s*([!?.]\\s*))+', r' \\1 ', text)\n",
    "    text = re.sub(r\"_+\", \" \", text)\n",
    "\n",
    "    # 8. Linguistic corrections\n",
    "    text = contractions.fix(text)          # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    text = chat_words_conversion(text)     # Convert chat abbreviations (e.g., \"u\" -> \"you\")\n",
    "\n",
    "    # 9. Final cleanup: lowercase, strip boundary <user>, normalize spaces\n",
    "    text = text.lower()\n",
    "    text = remove_boundary_users(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"you are a #loser @user ! see you tomorrow with 10 euros !!! and .. jean =you owe me 10k https://example.com ðŸ˜Š #fun, #lol\"\n",
    "print(prepro(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "507e8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16851/16851 [00:01<00:00, 11762.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values in the 'Text' column with an empty string\n",
    "df[\"Text\"] = df[\"Text\"].fillna(\"\")\n",
    "\n",
    "# Apply the preprocessing function\n",
    "df[\"Text\"] = df[\"Text\"].progress_apply(prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "2ba25114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16851/16851 [00:00<00:00, 90926.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Glued tokens\n",
    "def unpack_glued_tokens(text):\n",
    "    glued_token_mapping = {\n",
    "    \"adviceforyoungfeminists\": \"advice for young feminists\",\n",
    "    \"allfemale\": \"all female\",\n",
    "    \"cuntandandre\": \"cunt and andre\",\n",
    "    \"dudebros\": \"dude bros\",\n",
    "    \"femfreefriday\": \"feminist free friday\",\n",
    "    \"feministsareugly\": \"feminists are ugly\",\n",
    "    \"femshep\": \"feminist shep\",\n",
    "    \"hatefilled\": \"hate filled\",\n",
    "    \"heforshe\": \"he for she\",\n",
    "    \"ideaology\": \"ideology\",\n",
    "    \"islamofascist\": \"islamo fascist\",\n",
    "    \"islamofascists\": \"islamo fascists\",\n",
    "    \"islamolunatic\": \"islamo lunatic\",\n",
    "    \"islamolunatics\": \"islamo lunatics\",\n",
    "    \"killerblondes\": \"killer blondes\",\n",
    "    \"likeagirl\": \"like a girl\",\n",
    "    \"murderbigotry\": \"murder bigotry\",\n",
    "    \"nonmuslims\": \"non muslims\",\n",
    "    \"notsexist\": \"not sexist\",\n",
    "    \"questionsformen\": \"questions for men\",\n",
    "    \"redscarebot\": \"red scare bot\",\n",
    "    \"selfawareness\": \"self awareness\",\n",
    "    \"sorryitsaboy\": \"sorry it is a boy\",\n",
    "    \"womenagainstfeminism\": \"women against feminism\",\n",
    "    \"yesallwomen\": \"yes all women\",\n",
    "    \"idontneedfeminism\": \"i do not need feminism\",\n",
    "    \"amirite\": \"am i right\",\n",
    "    \"everydaysexism\": \"every day sexism\",\n",
    "    \"yearolds\": \"year old\",\n",
    "    \"yearold\": \"year old\",\n",
    "    \"yrold\": \"year old\",\n",
    "    \"allmale\": \"all male\",\n",
    "    \"feminazi\": \"feminist nazi\",\n",
    "    \"gangraped\": \"gang raped\",\n",
    "    \"nonmuslim\": \"non muslim\",\n",
    "    \"nosexist\": \"not sexist\",\n",
    "    \"promogirls\": \"promote girls\",\n",
    "    \"tweetlikeafeminist\": \"tweet like a feminist\",\n",
    "    \"twitterfeminism\": \"twitter feminism\",\n",
    "    'letstalkmen': 'lets talk men'\n",
    "    }\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(glued_token_mapping.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda m: glued_token_mapping[m.group(0)], text)\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].progress_apply(unpack_glued_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16851/16851 [00:01<00:00, 15432.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_orthographic_errors(text):\n",
    "    orthographic_mapping = {\n",
    "        'allahs': 'allah',\n",
    "        'anamists': 'animist',\n",
    "        'apostacy': 'apostasy',\n",
    "        'biatch': 'bitch',\n",
    "        'biggots': 'bigot',\n",
    "        'blumenthals': 'blumenthal',\n",
    "        'christiandom': 'christian',\n",
    "        'colonialized': 'colonized',\n",
    "        'dck': 'dick',\n",
    "        'deash': 'daesh',\n",
    "        'douch': 'douche',\n",
    "        'femal': 'female',\n",
    "        'gende': 'gender',\n",
    "        'hamaz': 'hamas',\n",
    "        'islams': 'islam',\n",
    "        'kuffir': 'kafir',\n",
    "        'kunt': 'cunt',\n",
    "        'mohammeds': 'mohammed',\n",
    "        'pedophelia': 'pedophilia',\n",
    "        'probs': 'probably',\n",
    "        'punnished': 'punished',\n",
    "        'sammich': 'sandwich',\n",
    "        'sexists': 'sexist',\n",
    "        'shutup': 'shut up',\n",
    "        'taquiyya': 'taqiyya',\n",
    "        'tradie': 'worker',\n",
    "        'wheras': 'whereas',\n",
    "        'spatchcock': 'chicken',\n",
    "        'wayyy': 'way',\n",
    "        'faaark': 'fuck',\n",
    "        'ablazing': 'blazing',\n",
    "        'aggres': 'aggressive',\n",
    "        'balistic': 'ballistic',\n",
    "        'blaspemy': 'blasphemy',\n",
    "        'budhists': 'buddhists',\n",
    "        'burrying': 'burying',\n",
    "        'carnt': \"can not\",\n",
    "        'cmon': 'come on', \n",
    "        'digusting': 'disgusting',\n",
    "        'excitin': 'exciting',\n",
    "        'fkn': 'fucking',\n",
    "        'genuinly': 'genuinely',\n",
    "        'judism': 'judaism',\n",
    "        'litteraly': 'literally',\n",
    "        'mohommed': 'mohammed',\n",
    "        'palistine': 'palestine',\n",
    "        'peacful': 'peaceful',\n",
    "        'percieved': 'perceived',\n",
    "        'wemon': 'women',\n",
    "        'judism': 'judaism',\n",
    "        'annoyi': 'annoying',\n",
    "        'aparthide': 'apartheid',\n",
    "        'apharthide': 'apartheid',\n",
    "        'areseholes': 'asshole',\n",
    "        'arseholes': 'asshole',\n",
    "        'argmnt': 'argument',\n",
    "        'womem': 'women',\n",
    "        'isil': 'isis',\n",
    "        'islamophobe': 'islam hate',\n",
    "        'retweet': 'tweet'\n",
    "    }\n",
    "    \n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in orthographic_mapping.keys()) + r')\\b')\n",
    "    \n",
    "    # replace function to handle case insensitivity\n",
    "    def replace_match(match):\n",
    "        matched_word = match.group(0)\n",
    "        lower_word = matched_word.lower()\n",
    "        replacement = orthographic_mapping[lower_word]\n",
    "        return replacement\n",
    "    \n",
    "    return pattern.sub(replace_match, text)\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].progress_apply(correct_orthographic_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85cdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16851/16851 [00:00<00:00, 27018.85it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_final_mapping(text):\n",
    "    final_token_mapping = {\n",
    "        # Expressive/emotional tokens\n",
    "        'arghhhhh': 'angry',\n",
    "        'gaaaaah': 'fuck', \n",
    "        'grrrrrr': 'grr',\n",
    "        'hahaha': 'haha',\n",
    "        'hahahaha': 'haha',\n",
    "        'lololol': 'lol',\n",
    "        'roflmao': 'lol',\n",
    "        'ehhhh': 'disgusting',\n",
    "        \n",
    "        # Gender-related terms\n",
    "        'bimbolines': 'bimbo',\n",
    "        'fems': 'feminists',\n",
    "        'misandrist': 'man hater',\n",
    "        'pussies': 'cowards',\n",
    "        'radfems': 'feminists',\n",
    "        'sjws': 'social justice activists',\n",
    "        \n",
    "        # Insults/profanity\n",
    "        'dipwad': 'idiot',\n",
    "        'douchebag': 'jerk', \n",
    "        'dumbass': 'fool',\n",
    "        'horseshit': 'nonsense',\n",
    "        'microbrain': 'idiot',\n",
    "        'skanks': 'bitch',\n",
    "        \n",
    "        # Political/sensitive terms\n",
    "        'daesh': 'isis',\n",
    "        'daeshbag': 'terrorist',\n",
    "        'daeshbags': 'terrorists',\n",
    "        'gamergate': 'gamer controversy',\n",
    "        'naziphobia': 'fear of nazi',\n",
    "        'tcot': 'top conservatives',\n",
    "        'mras': 'mens rights activists',\n",
    "        \n",
    "        # Ethnic terms\n",
    "        'ezidi': 'yazidi',\n",
    "        'ezidis': 'yazidis',\n",
    "        \n",
    "        # Miscellaneous\n",
    "        'deadset': 'determined',\n",
    "        'krazyeyes': 'crazy',\n",
    "        'mannnn': 'man',\n",
    "        'nomorepage': 'feminist controversy',\n",
    "        'selfie': 'self picture',\n",
    "        'spatchcock': 'chicken',\n",
    "        'unfollow': 'stop following'\n",
    "    }\n",
    "    \n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in final_token_mapping.keys()) + r')\\b')\n",
    "    \n",
    "    # Fonction de remplacement qui prÃ©serve la casse\n",
    "    def replace_match(match):\n",
    "        matched_word = match.group(0)\n",
    "        lower_word = matched_word.lower()\n",
    "        replacement = final_token_mapping[lower_word]\n",
    "        return replacement\n",
    "    \n",
    "    return pattern.sub(replace_match, text)\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].progress_apply(correct_final_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only sentences with more than 4 words without counting <user> token\n",
    "df = df[df[\"Text\"].apply(lambda x: len([word for word in x.split() if word != \"<user>\"]) >= 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "9dd0b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.drop(['id', 'index', 'oh_label'], axis=1)\n",
    "df_cleaned = df_cleaned.dropna(subset=['Annotation'])\n",
    "df_cleaned = df_cleaned.rename(columns={'Annotation': 'label', 'Text': 'text'})\n",
    "\n",
    "#reset index\n",
    "df_cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "61fbe33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "none      10387\n",
       "sexism     3258\n",
       "racism     1965\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset will be used for the training of our LaTextGAN model.\n",
    "\n",
    "df_cleaned.to_pickle(\"path/Dataset/New_Preprocessed_Dataset_GAN.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb6d7e",
   "metadata": {},
   "source": [
    "## SBERT Embedding for cyberbullying detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7afc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_pickle(\"path/Dataset/New_Preprocessed_Dataset_GAN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user> tell you in no context lol\n"
     ]
    }
   ],
   "source": [
    "# completely remove punctuation and keep one space between words\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation and replace with space\n",
    "    text = re.sub(r'[^\\w\\s<>]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "# usage example\n",
    "text = \"<user> tell you in no.context, lol !\"\n",
    "print(remove_punctuation(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "63758c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['text'] = df_cleaned['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sbert\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "852c5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbert = df_cleaned.copy()\n",
    "df_sbert['embedding'] = df_sbert['text'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfc7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset will be used for the classification task\n",
    "\n",
    "df_sbert.to_pickle(\"path/Dataset/New_Preprocessed_Dataset.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
