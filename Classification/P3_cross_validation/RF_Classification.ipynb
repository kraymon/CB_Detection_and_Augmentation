{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# additional libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pandas import read_pickle\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, StratifiedKFold\n",
    ")\n",
    "\n",
    "# Imbalanced-learn \n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('path/Dataset/New_Preprocessed_Dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd65549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename label : none to normal\n",
    "df['label'] = df['label'].replace('none', 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0344ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(df['embedding'].tolist()) \n",
    "labels = df['label'].tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6410d",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_with_gridsearch(X_train, y_train, X_test, y_test):\n",
    "    # grid search parameters for Random Forest\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [50],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        rf_classifier,\n",
    "        param_grid=param_grid_rf,\n",
    "        cv=cv,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_with_oversampling(X, y, oversampler, train_fn, n_splits=5, class_names=[\"normal\", \"racism\", \"sexism\"]):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_f1_scores = []\n",
    "    all_accuracies = []\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_conf_matrices = []\n",
    "    best_params_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\nFold {fold} ----------------------------\")\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "        # Oversampling\n",
    "        X_train_over, y_train_over = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Model training\n",
    "        grid = train_fn(X_train_over, y_train_over, X_val, y_val)\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = best_model.predict(X_val)\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        all_accuracies.append(acc)\n",
    "        all_precisions.append(prec)\n",
    "        all_recalls.append(rec)\n",
    "        all_f1_scores.append(f1)\n",
    "        best_params_list.append(grid.best_params_)\n",
    "\n",
    "        print(f\"Accuracy : {acc:.4f}\")\n",
    "        print(f\"Precision : {prec:.4f}\")\n",
    "        print(f\"Recall : {rec:.4f}\")\n",
    "        print(f\"F1 macro : {f1:.4f}\")\n",
    "        print(classification_report(y_val, y_pred, zero_division=0, target_names=class_names))\n",
    "\n",
    "        # Confusion matrix (raw)\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        all_conf_matrices.append(cm)\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix (Fold {fold})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "    # Final summary \n",
    "    print(\"\\nFinal Cross-Validation Summary:\")\n",
    "\n",
    "    mean_cm = np.mean(all_conf_matrices, axis=0).astype(int)\n",
    "    cm_percent = np.round(mean_cm / mean_cm.sum(axis=1, keepdims=True) * 100, 1)\n",
    "\n",
    "    # Raw average CM\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(mean_cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Mean Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    # Percentage CM\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Mean Confusion Matrix (Percentages)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean Accuracy : {np.mean(all_accuracies):.4f}\")\n",
    "    print(f\"Mean Precision : {np.mean(all_precisions):.4f}\")\n",
    "    print(f\"Mean Recall : {np.mean(all_recalls):.4f}\")\n",
    "    print(f\"Mean F1 macro : {np.mean(all_f1_scores):.4f}\")\n",
    "    print(f\"Â± Std F1 macro : {np.std(all_f1_scores):.4f}\")\n",
    "\n",
    "    return all_f1_scores, best_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1608473",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "f1s_ros, params_ros = cross_val_with_oversampling(embeddings, labels, train_fn=train_rf_with_gridsearch, oversampler=ros, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8319c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "f1s_smote, params_smote = cross_val_with_oversampling(embeddings, labels, train_fn=train_rf_with_gridsearch, oversampler=smote, n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f82371",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36701bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_with_gan(X, y, df_gan, label=\"GAN\", n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    df_gan = df_gan.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    gan_chunks = np.array_split(df_gan, n_splits)\n",
    "\n",
    "    f1_scores, acc_scores, prec_scores, rec_scores = [], [], [], []\n",
    "    cm_matrices_pct = []\n",
    "    cm_matrices_raw = []\n",
    "    param_list = []\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\nFold {i}/{n_splits}\")\n",
    "\n",
    "        # Split\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = np.array(y)[train_idx], np.array(y)[test_idx]\n",
    "\n",
    "        # GAN augmentation\n",
    "        df_gan_fold = gan_chunks[i-1]\n",
    "        X_gan = np.array(df_gan_fold['embedding'].tolist())\n",
    "        y_gan = np.array(df_gan_fold['label'].tolist())\n",
    "\n",
    "        X_train_gan = np.concatenate((X_train, X_gan), axis=0)\n",
    "        y_train_gan = np.concatenate((y_train, y_gan), axis=0)\n",
    "\n",
    "        # Training\n",
    "        grid = train_rf_with_gridsearch(X_train_gan, y_train_gan, X_test, y_test)\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Scores\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        acc_scores.append(acc)\n",
    "        prec_scores.append(prec)\n",
    "        rec_scores.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "        param_list.append(grid.best_params_)\n",
    "\n",
    "        print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 (macro): {f1:.4f}\")\n",
    "\n",
    "        # Confusion matrix (raw and percentage)\n",
    "        cm_raw = confusion_matrix(y_test, y_pred)\n",
    "        cm_matrices_raw.append(cm_raw)\n",
    "\n",
    "        cm_pct = (cm_raw / cm_raw.sum(axis=1, keepdims=True)) * 100\n",
    "        cm_matrices_pct.append(cm_pct)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        disp = ConfusionMatrixDisplay(cm_raw, display_labels=np.unique(y))\n",
    "        disp.plot(ax=ax, cmap='Blues', values_format=\"d\", colorbar=False)\n",
    "        ax.set_title(f\"Confusion Matrix - Fold {i}\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "    # Average matrices\n",
    "    mean_cm_raw = np.mean(cm_matrices_raw, axis=0).round().astype(int)\n",
    "    mean_cm_pct = np.mean(cm_matrices_pct, axis=0).round(2)\n",
    "\n",
    "    # Mean raw matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    disp = ConfusionMatrixDisplay(mean_cm_raw, display_labels=np.unique(y))\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format=\"d\", colorbar=False)\n",
    "    ax.set_title(f\"Mean Confusion Matrix - {label}\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    # Mean percentage matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    disp = ConfusionMatrixDisplay(mean_cm_pct, display_labels=np.unique(y))\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format=\".2f\", colorbar=False)\n",
    "    ax.set_title(f\"Mean Confusion Matrix (Percentage) - {label}\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    # Score summary\n",
    "    print(f\"\\nScore summary - {label}\")\n",
    "    print(f\"Accuracy   : {np.mean(acc_scores):.4f}\")\n",
    "    print(f\"Precision  : {np.mean(prec_scores):.4f}\")\n",
    "    print(f\"Recall     : {np.mean(rec_scores):.4f}\")\n",
    "    print(f\"F1 (macro) : {np.mean(f1_scores):.4f}\")\n",
    "    print(f\"F1 Std Dev : {np.std(f1_scores):.4f}\")\n",
    "\n",
    "    return f1_scores, param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gan = read_pickle('path/Dataset/GAN_DF.pkl')\n",
    "\n",
    "f1s_gan, params_gan = cross_val_with_gan(embeddings, labels, df_gan, n_splits=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5cc11",
   "metadata": {},
   "source": [
    "### Without Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25494f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_without_oversampling(X, y, train_fn, n_splits=5, class_names=[\"normal\", \"racism\", \"sexism\"]):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_f1_scores = []\n",
    "    all_accuracies = []\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_conf_matrices = []\n",
    "    best_params_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\nFold {fold} ----------------------------\")\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "        # Model training\n",
    "        grid = train_fn(X_train, y_train, X_val, y_val)\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = best_model.predict(X_val)\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        all_accuracies.append(acc)\n",
    "        all_precisions.append(prec)\n",
    "        all_recalls.append(rec)\n",
    "        all_f1_scores.append(f1)\n",
    "        best_params_list.append(grid.best_params_)\n",
    "\n",
    "        print(f\"Accuracy : {acc:.4f}\")\n",
    "        print(f\"Precision : {prec:.4f}\")\n",
    "        print(f\"Recall : {rec:.4f}\")\n",
    "        print(f\"F1 macro : {f1:.4f}\")\n",
    "        print(classification_report(y_val, y_pred, zero_division=0, target_names=class_names))\n",
    "\n",
    "        # Confusion matrix (raw)\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        all_conf_matrices.append(cm)\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix (Fold {fold})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\nFinal Cross-Validation Summary:\")\n",
    "\n",
    "    mean_cm = np.mean(all_conf_matrices, axis=0).astype(int)\n",
    "    cm_percent = np.round(mean_cm / mean_cm.sum(axis=1, keepdims=True) * 100, 1)\n",
    "\n",
    "    # Raw average CM\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(mean_cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Mean Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    # Percentage CM\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Mean Confusion Matrix (Percentages)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean Accuracy : {np.mean(all_accuracies):.4f}\")\n",
    "    print(f\"Mean Precision : {np.mean(all_precisions):.4f}\")\n",
    "    print(f\"Mean Recall : {np.mean(all_recalls):.4f}\")\n",
    "    print(f\"Mean F1 macro : {np.mean(all_f1_scores):.4f}\")\n",
    "    print(f\"Â± Std F1 macro : {np.std(all_f1_scores):.4f}\")\n",
    "\n",
    "    return all_f1_scores, best_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s_no, params_no = cross_val_without_oversampling(embeddings, labels, train_fn=train_rf_with_gridsearch, n_splits=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
